---
title: CS229 Lec 2 
date: 2020-09-10 08:00
categories: [ML, CS229]
tags: [ML]
math: true
---


# Outline
- (Supervised) Linear Regression
- Batch / Stochastic Gradient Descent
- Normal Equation

# Symbols

$$ \theta $$      : Parameters

$$ m $$  : Number of training examples or Number of rows in table

$$ x $$  : Inputs or Features

$$ y $$  : Output or Target variable

$$ \left( x, y \right) $$   : a training example

$$ \left( x^\left(i\right), y^\left(i\right) \right) $$   : i-th training example

$$ n $$  : Numbmer of features or Dimension of training examples


# Linear Regression

Supervised Learning의 한 예

training set을 learning algorithm을 통해서 "hypothesis" $$h$$를 만드는 과정이다.

## How to represent hypothesis

Learning algorithm을 설계할 때 고려해야 할 것 중 하나는 $$ h $$ 를 표현하는 방법이다.

Linear Regression에서는 $$ h $$ 를 이렇게 표현한다.

$$ h(x) = \sum_{i=1}^n \theta_i x_i \ \  where \ \  x_0 = 1 $$

<center> 또는 </center>

$$ h(x) = \theta^T x  \ \  where \ \  \theta \in \mathbb{R}^{n+1}, \  x \in \mathbb{R}^{n+1} $$

참고로 $$h(x)$$는 $$h_\theta(x)$$와 동일한 표현이다.

## How to choose parameters

$$ Choose\  \theta \ s.t.\  h(x) \approx y \ \ for \ training\ examples $$

추정치 $$ h(x) $$와 실제 값 $$ y $$ 의 차이를 줄여야한다는 것이고 이를 이용해서 새로운 함수를 정의한다.

$$ minimize \ \  J\left( \theta \right) = 1/2 \sum_{i=1}^m\left(  h_\theta\left( x^\left( i \right) \right) - y^\left( i \right)  \right)^2 $$

이때 $$ J\left(\theta \right) $$를 cost function이라 한다.

$$J(\theta)$$(cost function)이 최소가 되는 $$\theta$$를 찾으면 된다.

## How to find parameters (Gradient Descent Algorithm)

### Basic Algorithm

1. Start with some $$\theta$$    (eg. $$\theta = \vec{0} $$)

2. Keep changing $$\theta$$ to reduce $$J(\theta)$$

3. Repead until convergence for $$j=0, 1, 2, \cdots, n$$

4. 　　$$ \theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j} J(\theta) \ \ where\ \alpha\ is\ learning\ rate$$

### Batch Gradient Algorithm

#### Formula

$$ \theta_j := \theta_j - \alpha \sum_{i=1}^m( h_\theta(x^\left(i\right)) - y^\left(i\right) ) x_j^\left(i \right) $$

#### Disadvantage

$$m$$이 큰 수일때, 매번 $$\theta_j$$을 업데이트 할 때마다 모든 수를 계산하므로 시간이 오래걸린다.  

#### Proof

later...

### Stochastic Gradient Algorithm

#### Formula

1. Repeat 

2. 　for $$i=1$$ to $$m$$

3. 　　$$ \theta_j := \theta_j - \alpha (h_\theta(x^\left(i\right)) - y^\left(i\right))x_j^\left(i\right) $$

#### Features

$$\theta_j$$를 한개의 training example에 대해서만 업데이트함.

$$h(x)$$는 보통 global minimum으로 수렴하고 더 빠르다.

large data set에 대해서 효과적이다.

다만, 업데이트 연산에 한개의 training example만 사용하므로 global minimum에 정확히 도달하지는 않는다.

## Normal Equation

Linear Regression에만 적용 가능

한번의 계산으로 optimal value에 도달할 수 있다.

Linear Regression인 경우에 $$J(\theta)$$는 $$x$$에 대한 2차방정식이므로

local optimum == global optimum이 된다.

### Formula

$$ \theta = (x^Tx)^{-1}x^Ty \ \ where\ x \in \mathbb{R}^{n+1},\ y \in \mathbb{R}^{n+1} $$

### Proof

later...